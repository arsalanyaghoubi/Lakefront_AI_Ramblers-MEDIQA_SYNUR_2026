`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:01<00:37,  1.30s/it]Loading checkpoint shards:   7%|▋         | 2/30 [00:03<00:47,  1.71s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:05<00:51,  1.91s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:07<00:52,  2.01s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:09<00:50,  2.02s/it]Loading checkpoint shards:  20%|██        | 6/30 [00:11<00:48,  2.03s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [00:13<00:46,  2.03s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [00:15<00:45,  2.08s/it]Loading checkpoint shards:  30%|███       | 9/30 [00:18<00:44,  2.10s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [00:20<00:41,  2.08s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [00:22<00:39,  2.07s/it]Loading checkpoint shards:  40%|████      | 12/30 [00:24<00:37,  2.06s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [00:26<00:35,  2.09s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [00:28<00:33,  2.12s/it]Loading checkpoint shards:  50%|█████     | 15/30 [00:30<00:31,  2.09s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:32<00:29,  2.07s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:34<00:26,  2.06s/it]Loading checkpoint shards:  60%|██████    | 18/30 [00:36<00:25,  2.09s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:39<00:23,  2.13s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:41<00:21,  2.15s/it]Loading checkpoint shards:  70%|███████   | 21/30 [00:43<00:20,  2.27s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:46<00:20,  2.53s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:50<00:20,  2.88s/it]Loading checkpoint shards:  80%|████████  | 24/30 [00:52<00:16,  2.73s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:55<00:13,  2.69s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:58<00:11,  2.82s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [01:02<00:09,  3.17s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [01:07<00:07,  3.58s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [01:11<00:03,  3.85s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:11<00:00,  2.77s/it]Loading checkpoint shards: 100%|██████████| 30/30 [01:11<00:00,  2.40s/it]
You are trying to use a model that was created with Sentence Transformers version 5.2.2, but you're currently using version 5.1.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.
  0%|          | 0/199 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/data/users/msaban/.conda/envs/synur/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  1%|          | 1/199 [04:10<13:46:42, 250.52s/it]  1%|          | 2/199 [06:10<9:30:55, 173.89s/it]   2%|▏         | 3/199 [10:32<11:38:47, 213.91s/it]slurmstepd: error: *** JOB 3170 ON cn7 CANCELLED AT 2026-02-06T04:20:54 ***
